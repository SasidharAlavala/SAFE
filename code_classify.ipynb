{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Bleeding and Non-Bleeding Frames in Wireless Capsule Endoscopy Using Swin Transformer\n",
    "This code classifies \"Bleeding\" and \"Non-Bleeding\" frames in Wireless Capsule Endoscopy using a Swin Transformer model. It includes data preprocessing, MixUp and various augmentations, and training with AdamW and cosine annealing. After training, it evaluates accuracy, generates Ablation-CAM heatmaps, and saves predictions to an Excel file. This approach automates gastrointestinal disorder diagnosis from endoscopy videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import random \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.cuda.amp as amp \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pytorch_grad_cam import AblationCAM\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerVit\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from swin_transformer import SwinTransformer\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Augmentation\n",
    "- **rgb_to_lab_and_clahe function** - Performs conversion of RGB image to LAB color space after blurring (Gaussian) the Green (G) AND Blue (B) channels to emphasize the R channel and performs CLAHE. \n",
    "- **Augmentation** - Transforms like random Horizontal flip, Vertical flip, Rotation, Gaussian Blur, Affine, Perspective projection & MixUp are applied to create diversity in the training dataset to enhance model training and improve generalization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_lab_and_clahe(img, clip_limit, tile_grid_size, blur_sigma):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to LAB color space, apply CLAHE (Contrast Limited Adaptive Histogram Equalization).\n",
    "\n",
    "    Args:\n",
    "        img (PIL.Image): Input RGB image.\n",
    "        clip_limit (float): Clip limit for CLAHE.\n",
    "        tile_grid_size (tuple): Size of the tile grid for CLAHE.\n",
    "        blur_sigma (float): Standard deviation for Gaussian blur.\n",
    "\n",
    "    Returns:\n",
    "        lab_image (numpy.ndarray): LAB color space image.\n",
    "    \"\"\"        \n",
    "    rgb_r, rgb_g, rgb_b = cv2.split(np.array(img))\n",
    "    blurred_G = cv2.GaussianBlur(rgb_g, (5, 5), blur_sigma)\n",
    "    blurred_B = cv2.GaussianBlur(rgb_b, (5, 5), blur_sigma)\n",
    "\n",
    "    img = cv2.merge([rgb_r, blurred_G, blurred_B]) \n",
    "    lab_image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2Lab)\n",
    "    L, a, b = cv2.split(lab_image)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    clahed_L = clahe.apply(L)  \n",
    "    modified_lab_image = cv2.merge([clahed_L, a, b])\n",
    "    rgb_image = cv2.cvtColor(modified_lab_image, cv2.COLOR_LAB2RGB)\n",
    "    return lab_image \n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=180), \n",
    "    transforms.GaussianBlur(kernel_size=5, sigma=(1.0, 3.0)),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.25, 0.25), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    #lambda x: rgb_to_lab_and_clahe(x, clip_limit=5.0, tile_grid_size=(8, 8), blur_sigma=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),   \n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    #lambda x: rgb_to_lab_and_clahe(x, clip_limit=5.0, tile_grid_size=(8, 8), blur_sigma=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),  \n",
    "])\n",
    "\n",
    "def mixup(data, targets, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Applies MixUp augmentation to the input data and targets.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Input data tensor.\n",
    "        targets (torch.Tensor): Target labels tensor.\n",
    "        alpha (float): MixUp hyperparameter controlling the mix ratio.\n",
    "\n",
    "    Returns:\n",
    "        mixed_data (torch.Tensor): Mixed data tensor.\n",
    "        targets_a (torch.Tensor): Targets for the first data sample.\n",
    "        targets_b (torch.Tensor): Targets for the second data sample.\n",
    "        lam (float): Lambda value representing the mix ratio.\n",
    "    \"\"\"\n",
    "    if alpha == 0:\n",
    "        return data, targets, targets, 1.0 \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = data.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_data = lam * data + (1 - lam) * data[index, :]\n",
    "    targets_a, targets_b = targets, targets[index]\n",
    "    return mixed_data, targets_a, targets_b, lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device for PyTorch to CUDA if available, otherwise use CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set a random seed for reproducibility and configure CUDA for benchmarking.\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Define the file paths for the training and validation datasets.\n",
    "train_data_path = '/data2/user123/c2/c2_1_0/data/data_classify/train'\n",
    "val_data_path = '/data2/user123/c2/c2_1_0/data/data_classify/val'\n",
    "\n",
    "# Create PyTorch datasets with specified transformations.\n",
    "train_dataset = ImageFolder(train_data_path, transform=train_transform)\n",
    "val_dataset = ImageFolder(val_data_path, transform=val_transform)\n",
    "\n",
    "# Set the batch size and create data loaders for training and validation.\n",
    "batch_size = 24\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Criterion & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Swin Transformer model configuration.\n",
    "swin_config = {\n",
    "    'img_size': 224,\n",
    "    'patch_size': 4,\n",
    "    'in_chans': 3,\n",
    "    'num_classes': 2,\n",
    "    'embed_dim': 96,\n",
    "    'depths': [2, 2, 18, 2],\n",
    "    'num_heads': [3, 6, 12, 24],\n",
    "    'window_size': 7,\n",
    "    'mlp_ratio': 4,\n",
    "    'stochastic_depth_prob': 0.3,\n",
    "}\n",
    "\n",
    "# Instantiate Swin Transformer model. \n",
    "model = SwinTransformer(img_size=swin_config['img_size'],\n",
    "                        patch_size=swin_config['patch_size'],\n",
    "                        in_chans=swin_config['in_chans'],\n",
    "                        num_classes=swin_config['num_classes'],\n",
    "                        embed_dim=swin_config['embed_dim'],\n",
    "                        depths=swin_config['depths'],\n",
    "                        num_heads=swin_config['num_heads'],\n",
    "                        window_size=swin_config['window_size'],\n",
    "                        mlp_ratio=swin_config['mlp_ratio'],\n",
    "                        qkv_bias=True,\n",
    "                        qk_scale=None,\n",
    "                        drop_rate=0.0,\n",
    "                        drop_path_rate=0.1,\n",
    "                        ape=False,\n",
    "                        patch_norm=True,\n",
    "                        use_checkpoint=False,\n",
    "                        fused_window_process=False).to(device)\n",
    "\n",
    "# Generate a summary of the model's architecture\n",
    "summary(model, (3, 224, 224)) \n",
    "\n",
    "# Set up the Cross Entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the Optimizer as AdamW\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=7.8125e-06,     \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1.0e-08,      \n",
    "    weight_decay=0.05,   \n",
    ")\n",
    "\n",
    "# Set the Scheduler\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=30,   \n",
    "    eta_min=7.8125e-08  \n",
    ")\n",
    "\n",
    "# Set gradient scaler\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training parameters and lists to track metrics.\n",
    "num_epochs = 250\n",
    "best_accuracy = 0.0\n",
    "best_recall = 0.0\n",
    "best_f1_score = 0.0\n",
    "best_epoch_accuracy = 0\n",
    "best_model_path = '/data2/user123/c2/c2_1_0/code/save'\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "# Main training loop over epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop over batches.\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Apply MixUp augmentation.\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, targets_a, targets_b, lam = mixup(images, labels, alpha=1.0)\n",
    "        else:\n",
    "            images, targets_a, targets_b, lam = mixup(images, labels, alpha=1.0)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with amp.autocast():\n",
    "            outputs = model(images)\n",
    "            #loss = criterion(outputs, labels)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        #scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(\"############################################################\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss/len(train_loader)}, Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        # Validation loop to evaluate model performance.\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                val_loss = criterion(outputs, labels)   \n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_losses.append(running_val_loss/len(val_loader))\n",
    "\n",
    "        # Print and save model performance metrics.\n",
    "        print(f\"Val Loss: {running_val_loss/len(val_loader)}\")\n",
    "        val_accuracies.append(accuracy)\n",
    "        recall = recall_score(val_labels, val_predictions)\n",
    "        f1 = f1_score(val_labels, val_predictions)\n",
    "\n",
    "        print(f\"Current Validation Accuracy: {accuracy}%\")\n",
    "        print(f\"Current Validation Recall: {recall}\")\n",
    "        print(f\"Current Validation F1 Score: {f1}\")\n",
    "        print(f\"Best Validation Accuracy: {best_accuracy}% at Epoch {best_epoch}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_epoch = epoch + 1\n",
    "            best_accuracy = accuracy\n",
    "            model_path = os.path.join(best_model_path, f\"model_classify_swin.pth\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"Best Model Saved!\")\n",
    "\n",
    "# Plot training and validation metrics.\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, num_epochs + 1, 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(2, num_epochs + 1, 2), val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(2, num_epochs + 1, 2), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Dataloader for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining data preprocessing for test data.\n",
    "test_transform = transforms.Compose([\n",
    "    #lambda x: rgb_to_lab_and_clahe(x, clip_limit=5.0, tile_grid_size=(8, 8), blur_sigma=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
    "])\n",
    "\n",
    "#Defining path for saving test prediction, model checkpoint, CAMs, and Excel file\n",
    "test_data_path = '/data2/user123/c2/c2_1_0/data/data_classify/test'\n",
    "model_path = '/data2/user123/c2/c2_1_0/code/save/best_model_classify.pth'  \n",
    "cams_path = '/data2/user123/c2/c2_1_0/code/save/cams/'\n",
    "excel_file_path = '/data2/user123/c2/c2_1_0/code/save/predictions_classify.xlsx'\n",
    "batch_size = 1\n",
    "\n",
    "# Create a test dataset and data loader for inference.\n",
    "test_dataset = ImageFolder(test_data_path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and CAM plot generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to reshape a tensor for Ablation-CAM function \n",
    "def reshape_transform(tensor, height=7, width=7):\n",
    "    \"\"\"\n",
    "    Reshape a tensor to the specified height and width dimensions.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor.\n",
    "        height (int): Target height dimension.\n",
    "        width (int): Target width dimension.\n",
    "\n",
    "    Returns:\n",
    "        result (torch.Tensor): Reshaped tensor.\n",
    "    \"\"\"\n",
    "    result = tensor.reshape(tensor.size(0), height, width, tensor.size(2))\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result \n",
    "\n",
    "\n",
    "# Load a pre-trained model and set it to evaluation mode.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Define class names, fonts, and variables for predictions and image file names.\n",
    "class_names = ['bleeding', 'non_bleeding']\n",
    "i = 0\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.5\n",
    "font_color = (255, 255, 255)\n",
    "font_thickness = 1\n",
    "predicted_labels = []\n",
    "image_file_names = []\n",
    "serial = []\n",
    "\n",
    "# Loop through test data and generate predictions.\n",
    "for images, _ in test_loader:\n",
    "    images = images.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted_label = class_names[predicted.item()]\n",
    "\n",
    "# Uncomment this code for generating Ablation-CAM plots. \n",
    "    # target_layers = [model.layers[-1].blocks[-1].norm2]\n",
    "    # cam = AblationCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform, ablation_layer=AblationLayerVit())\n",
    "    # grayscale_cam = cam(input_tensor=images, aug_smooth=True, eigen_smooth=True, targets=None)\n",
    "    # rgb_img = cv2.imread(test_loader.dataset.samples[i][0], 1)[:, :, ::-1]\n",
    "    # rgb_img_n = np.float32(rgb_img) / 255\n",
    "    # cam_image = show_cam_on_image(rgb_img_n, grayscale_cam[0, :])\n",
    "\n",
    "    # predicted_labels.append(predicted_label)\n",
    "    # image_file_names.append(os.path.basename(test_loader.dataset.samples[i][0]))\n",
    "\n",
    "    # combined_image = np.copy(cam_image)\n",
    "    # text = f'Predicted: {predicted_label}'\n",
    "    # cv2.putText(combined_image, text, (15, 15), font, font_scale, font_color, font_thickness)\n",
    "    # cv2.imwrite(os.path.join(cams_path, os.path.basename(test_loader.dataset.samples[i][0])), combined_image)\n",
    "\n",
    "    ## Append predicted labels and image file names.\n",
    "    file_name = os.path.splitext(os.path.basename(test_loader.dataset.samples[i][0]))[0]\n",
    "    image_file_names.append(file_name+'.png')\n",
    "    predicted_labels.append(predicted.item())\n",
    "    i += 1\n",
    "    serial.append(i)\n",
    "\n",
    "# Create a DataFrame and save predictions to an Excel file.\n",
    "data = {\n",
    "    'S. No': serial,\n",
    "    'Image Name': image_file_names,\n",
    "    'Predicted Class label': predicted_labels,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['Predicted Class label'] = df['Predicted Class label'].map({0: 'Bleeding', 1: 'Non-Bleeding'})\n",
    "\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "print(f\"Predictions saved to {excel_file_path}\")\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
