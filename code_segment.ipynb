{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from torch import nn\n",
    "from monai.networks.nets import SwinUNETR\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from glob import glob\n",
    "import monai\n",
    "from monai import transforms\n",
    "from monai.transforms import MapTransform\n",
    "from monai import data\n",
    "from monai.metrics import DiceMetric\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.transforms import Compose, AsDiscrete, Activations\n",
    "\n",
    "def get_loader(batch_size):\n",
    "    train_img_dir = \"/data2/user123/c2/c2_1_0/data/data_segment/train\"\n",
    "    val_img_dir = \"/data2/user123/c2/c2_1_0/data/data_segment/val\"\n",
    "    train_ann_dir = \"/data2/user123/c2/c2_1_0/data/data_segment/train_ann\"\n",
    "    val_ann_dir = \"/data2/user123/c2/c2_1_0/data/data_segment/val_ann\"\n",
    "    \n",
    "    train_img_files = sorted(glob(os.path.join(train_img_dir, \"*\")))\n",
    "    train_ann_files = sorted(glob(os.path.join(train_ann_dir, \"*\"))) \n",
    "    train_files = [{\"image\": img, \"label\": ann} for img, ann in zip(train_img_files, train_ann_files)]\n",
    "\n",
    "    val_img_files = sorted(glob(os.path.join(val_img_dir, \"*\")))\n",
    "    val_ann_files = sorted(glob(os.path.join(val_ann_dir, \"*\")))\n",
    "    validation_files = [{\"image\": img, \"label\": ann} for img, ann in zip(val_img_files, val_ann_files)]\n",
    "\n",
    "    print(len(train_files), len(validation_files))\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            transforms.ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            transforms.ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = data.Dataset(data=train_files, transform=train_transform)\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_ds = data.Dataset(data=validation_files, transform=val_transform)\n",
    "    val_loader = data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Got CUDA!\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "  \n",
    "\n",
    "seed_val = 2\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "device = get_default_device()\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "'''' Initialise dataloaders '''\n",
    "path_save = \"/data2/user123/c2/c2_1_0/code/save\"\n",
    "batch_size = 64\n",
    "train_loader, val_loader = get_loader(batch_size)\n",
    "print(\"Dataloaders Initialised\") \n",
    "\n",
    "''' Initialise the model '''\n",
    "model = SwinUNETR(\n",
    "img_size=224,\n",
    "in_channels=3,\n",
    "out_channels=1,\n",
    "feature_size=48,\n",
    "drop_rate=0.0,\n",
    "attn_drop_rate=0.0,\n",
    "dropout_path_rate=0.0,\n",
    "use_checkpoint=True,\n",
    "spatial_dims=2).to(device)\n",
    "\n",
    "loss_function = monai.losses.DiceLoss(include_background=False,sigmoid=True)\n",
    "#loss_function1 = monai.losses.DiceCELoss(to_onehot_y=False, sigmoid=True, jaccard=True)\n",
    "step_size = len(train_loader) * 8\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "epoch_val_loss_values = []\n",
    "metric_values = []\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.4)])\n",
    "post_label = Compose([AsDiscrete(threshold=0.4)]) \n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time  \n",
    "\n",
    "    print(f\"Time taken for epoch {epoch + 1}: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            metric_sum =  0.0\n",
    "            metric_count =  0\n",
    "            \n",
    "            epoch_val_loss = 0\n",
    "            step = 0\n",
    "            for val_data in val_loader:\n",
    "                step += 1\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_inputs = val_inputs.permute(0, 3, 1, 2)\n",
    "                val_labels = val_labels.unsqueeze(1)\n",
    "                val_outputs = model(val_inputs)\n",
    "                loss = loss_function(val_outputs, val_labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_outputs = post_trans(val_outputs)\n",
    "                val_labels = post_label(val_labels)\n",
    "                # compute overall mean dice\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            epoch_val_loss /= step\n",
    "            epoch_val_loss_values.append(epoch_val_loss)\n",
    "            print(f\"epoch {epoch + 1} average val loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "            metric_values.append(metric)\n",
    "            \n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(path_save, \"model_segment_swinunetr.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Epoch Average Val Loss\")\n",
    "x1 = [2*i + 1 for i in range(len(epoch_val_loss_values))]\n",
    "y1 = epoch_val_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x1, y1, color=\"blue\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.transforms import Compose, Activations, AsDiscrete\n",
    "from monai.metrics import DiceMetric, compute_iou\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import LoadImaged, NormalizeIntensityd, ToTensord\n",
    "from monai.utils import set_determinism\n",
    "from glob import glob\n",
    "import monai\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "set_determinism(seed=42)\n",
    "\n",
    "# Define the paths\n",
    "model_path = \"/data2/user123/c2/c2_1_0/code/save/model_segment_swinunetr.pth\"\n",
    "test_img_dir = \"/data2/user123/c2/c2_1_0/data_org_test/Test Dataset 2/Images\"\n",
    "test_ann_dir = \"/data2/user123/c2/c2_1_0/data_org_test/Test Dataset 2/Annotations\"\n",
    "output_dir = \"/data2/user123/c2/c2_1_0/code/save/anns\"\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SwinUNETR(\n",
    "    img_size=224,\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    feature_size=12,\n",
    "    drop_rate=0.5,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.25,\n",
    "    use_checkpoint=True,\n",
    "    spatial_dims=2\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Define the transforms\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Create the test dataset and data loader\n",
    "test_files = sorted(glob(os.path.join(test_img_dir, \"*\")))\n",
    "test_labels = sorted(glob(os.path.join(test_ann_dir, \"*\")))\n",
    "test_data = [{\"image\": img, \"label\": ann} for img, ann in zip(test_files, test_labels)]\n",
    "test_dataset = monai.data.Dataset(data=test_data, transform=transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Define the metrics\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "image_names = []\n",
    "dice_scores = []\n",
    "iou_scores = []\n",
    "\n",
    "\n",
    "# Iterate over the test data and calculate the metrics\n",
    "for i, batch_data in enumerate(test_loader):\n",
    "    image_name = os.path.basename(test_files[i])\n",
    "    inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "    inputs = inputs.permute(0, 3, 1, 2)\n",
    "    labels = labels.unsqueeze(1)\n",
    "    outputs = model(inputs)\n",
    "    outputs = Activations(sigmoid=True)(outputs)\n",
    "    outputs = AsDiscrete(threshold=0.4)(outputs)\n",
    "    labels = AsDiscrete(threshold=0.4)(labels)\n",
    "    dice = dice_metric(y_pred=outputs, y=labels)\n",
    "    iou = compute_iou(y_pred=outputs, y=labels)\n",
    "    image_names.append(image_name)\n",
    "    dice_scores.append(dice.item())\n",
    "    iou_scores.append(iou.item())   \n",
    "\n",
    "    # Save the segmentation mask\n",
    "    mask = outputs.squeeze().detach().cpu().numpy()\n",
    "    mask_path = os.path.join(output_dir, f\"{image_name}\")\n",
    "    \n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Image\": image_names,\n",
    "    \"IoU\": iou_scores,\n",
    "    \"Dice\": dice_scores\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the average metrics\n",
    "average_dice = results_df[\"Dice\"].mean()\n",
    "average_iou = results_df[\"IoU\"].mean()\n",
    "\n",
    "# Save the results to an Excel sheet\n",
    "results_path = os.path.join(output_dir, \"results.xlsx\")\n",
    "results_df.to_excel(results_path, index=False)\n",
    "\n",
    "print(\"Dice and IoU calculation completed.\")\n",
    "print(f\"Average Dice: {average_dice:.4f}\")\n",
    "print(f\"Average IoU: {average_iou:.4f}\")\n",
    "print(f\"Results saved to: {results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasi_virus_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
